# import sys
# import os

# # ×”×•×¡×¤×ª ×ª×™×§×™×™×ª ×”×©×•×¨×© (CommandCore) ×œ× ×ª×™×‘ ×”×—×™×¤×•×© ×©×œ ×¤×™×™×ª×•×Ÿ
# sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# import streamlit as st
# from agents.agent_factory import SpecialistFactory
# from llm.llm_client import LLMClient

# def run_streamlit():
#     st.set_page_config(page_title="Command Core - Adaptive Lighthouse", layout="wide")
    
#     st.title("ğŸ›¡ï¸ Command Core - Milestone 1")
#     st.sidebar.header("Agent Selection")
    
#     # ××ª×—×•×œ ×”-LLM
#     if 'llm' not in st.session_state:
#         st.session_state.llm = LLMClient()
    
#     # ×‘×—×™×¨×ª ×¡×•×›×Ÿ ×œ×¤×™ ×“×¨×™×©×•×ª Milestone 1 [cite: 23, 26]
#     agent_options = {"Fire_Bot": "fire", "Police_Bot": "police", "Med_Bot": "medical"}
#     selected_name = st.sidebar.selectbox("Choose a Specialist:", list(agent_options.keys()))
#     agent_type = agent_options[selected_name]
    
#     # ×™×¦×™×¨×ª ×”×¡×•×›×Ÿ ×“×¨×š ×”-Factory
#     agent = SpecialistFactory.create(agent_type, st.session_state.llm)
    
#     st.write(f"### Chatting with: {agent.name} ({agent.role})")
#     st.info(f"**Persona:** {agent.persona}")

#     # × ×™×”×•×œ ×”×™×¡×˜×•×¨×™×™×ª ×”×¦'××˜
#     if "messages" not in st.session_state:
#         st.session_state.messages = []

#     for message in st.session_state.messages:
#         with st.chat_message(message["role"]):
#             st.markdown(message["content"])

#     # ×§×œ×˜ ××©×ª××© [cite: 27]
#     if prompt := st.chat_input("Describe the emergency situation..."):
#         st.session_state.messages.append({"role": "user", "content": prompt})
#         with st.chat_message("user"):
#             st.markdown(prompt)

#         with st.chat_message("assistant"):
#             with st.spinner(f"{agent.name} is analyzing..."):
#                 response = agent.analyze(prompt)
#                 st.markdown(response)
        
#         st.session_state.messages.append({"role": "assistant", "content": response})

# if __name__ == "__main__":
#     run_streamlit()

import streamlit as st
import sys
import os

# ×ª×™×§×•×Ÿ × ×ª×™×‘×™× (×›×¤×™ ×©×¢×©×™× ×• ×§×•×“×)
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from agents.agent_factory import SpecialistFactory
from llm.llm_client import LLMClient

def run_round_table():
    st.set_page_config(page_title="Command Core - Round Table", layout="wide")
    st.title("ğŸ¤ Milestone 2: The Round Table")
    
    if 'llm' not in st.session_state:
        st.session_state.llm = LLMClient()

    st.sidebar.info("In this mode, all agents will analyze the situation sequentially to coordinate their response.")

    # ×ª×™×‘×ª ×”×§×œ×˜ ×”××¨×›×–×™×ª
    if prompt := st.chat_input("Describe the emergency situation (e.g., Gas leak at school)..."):
        st.chat_message("user").markdown(prompt)
        
        # ×™×¦×™×¨×ª ×”×¡×•×›× ×™×
        factory = SpecialistFactory()
        fire = factory.create("fire", st.session_state.llm)
        police = factory.create("police", st.session_state.llm)
        medical = factory.create("medical", st.session_state.llm)
        
        agents = [fire, police, medical]
        shared_context = "" # ×–×”×• ×”-Shared Context ×©×œ Milestone 2

        # ×”×œ×•×œ××” ×”××©×•×ª×¤×ª (The Loop)
        for agent in agents:
            with st.chat_message(agent.name):
                with st.spinner(f"{agent.name} is coordinating..."):
                    # ×”×¡×•×›×Ÿ ××§×‘×œ ××ª ×”×§×œ×˜ + ××” ×©×§×“× ×œ×•
                    response = agent.analyze(prompt, shared_context)
                    st.markdown(response)
                    
                    # ×¢×“×›×•×Ÿ ×”×”×§×©×¨ ×”××©×•×ª×£ ×œ×”××©×š ×”×©×¨×©×¨×ª
                    shared_context += f"\n--- {agent.name} Proposal ---\n{response}\n"

if __name__ == "__main__":
    run_round_table()